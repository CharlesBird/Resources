input {
    file {
        path => "D:/BaiduNetdiskDownload/elastic/logstash/demo_data/earthquakes/earthquakes.csv"
        start_position => "beginning"
    }
}

filter {
#    mutate {
#        add_field => { "[@metadata][debug]" => true }
#    }

    csv {
        columns => ["timestamp","latitude","longitude","depth","mag","magType","nst","gap","dmin","rms","source","event_id"]
#        convert => {"latitude" => "float"}
#        convert => {"longitude" => "float"}
        convert => {"depth" => "float"}
        convert => {"mag" => "float"}
        convert => {"gap" => "float"}
        convert => {"dmin" => "float"}
        convert => {"rms" => "float"}
    }

#    mutate {
#        add_field => {
#            "location" => "%{latitude},%{longitude}"
#        }
#    }

    mutate {
        add_field => {
            "[location][lat]" => "%{latitude}"
            "[location][lon]" => "%{longitude}"
        }
    }

    mutate {
        convert => {
            "[location][lat]" => "float"
            "[location][lon]" => "float"
        }
    }

    # 2016/01/01 00:30:04.91
    date {
        match => ["timestamp","yyyy/MM/dd HH:mm:ss.SS"]

    }

    mutate {
        remove_field => ["latitude","longitude", "timestamp"]
    }

    mutate {
        add_field => {
            "[@metadata][index]" => "earthquakes"
        }
    }

    if [tags] {
        mutate {
            replace => {
                "[@metadata][index]" => "earthquakes_failure"
            }
        }
    }
}

output {
    if [@metadata][debug] {
        stdout {
            codec => rubydebug{metadata => true}
        }
    } else {
        stdout {codec => dots}

        elasticsearch{
            hosts => ["118.190.149.30:9200"]
            index => "%{[@metadata][index]}"
#            document_type => "_doc"
        }
    }
}